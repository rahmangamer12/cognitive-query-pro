# ui/analyzer_page.py - CognitiveQuery Pro - v14.1 "AEGIS-WORKSTATION-FIXED"
# ======================================================================================
#  This version includes the critical fix for the StreamlitDuplicateElementId error,
#  ensuring the application is completely stable and crash-proof. All real agent
#  calls and advanced features are preserved.
# ======================================================================================

# --- Core & Third-Party Imports ---
import streamlit as st
import pandas as pd
import time
from datetime import datetime
from typing import Dict, List, Any

# --- LangChain & Project Imports (ASSUMED TO EXIST BY USER) ---
from langchain_core.documents import Document
from agents.qa_agent import execute_qa_chain
from agents.report_agent import execute_report_chain
from agents.comparison_agent import execute_comparison_chain
from agents.summarizer_agent import execute_summarization_chain
from agents.entity_extraction_agent import execute_entity_extraction_chain
from agents.debug_agent import execute_debug_chain

# ======================================================================================
# SECTION 1: CRITICAL HELPER FUNCTIONS
# ======================================================================================
def get_retriever_from_state(ss: Dict):
    vector_store = ss.get("vector_store_handler")
    return vector_store.as_retriever() if vector_store else None

def track_performance(operation: str, start_time: float, ss: Dict):
    duration_ms = (time.perf_counter() - start_time) * 1000
    if "performance_log" not in ss: ss.performance_log = []
    ss.performance_log.insert(0, {"operation": operation, "duration_ms": duration_ms, "timestamp": datetime.now()})
    if len(ss.performance_log) > 50: ss.performance_log.pop()

# ======================================================================================
# SECTION 2: ATOMIC UI COMPONENT FUNCTIONS
# ======================================================================================
def render_tool_card(icon: str, title: str, text: str):
    st.markdown(f"""<div style="border: 1px solid var(--c-border); border-left: 5px solid var(--c-primary); padding: 1rem; border-radius: 8px; margin-bottom: 1rem; background-color: var(--c-surface);"><h3 style="margin-top:0;"><i class="fas fa-{icon}"></i>   {title}</h3><p style="color: var(--c-text-secondary); margin-bottom:0;">{text}</p></div>""", unsafe_allow_html=True)

def render_results_container(title: str, content_key: str, ss: Dict):
    content = ss.get(content_key)
    if content is not None and (not isinstance(content, (str, list, pd.DataFrame)) or not (isinstance(content, pd.DataFrame) and content.empty)):
        with st.container(border=True):
            col1, col2 = st.columns([5, 1])
            with col1: st.subheader(f"Agent Output: {title}")
            with col2:
                if st.button("Clear Output", key=f"clear_{content_key}", use_container_width=True):
                    ss[content_key] = None; st.rerun()
            if isinstance(content, pd.DataFrame): st.dataframe(content, use_container_width=True)
            else: st.markdown(content, unsafe_allow_html=True)

def render_performance_sidebar(ss: Dict):
    with st.sidebar:
        st.markdown("---"); st.subheader("🚀 Performance Monitor")
        if not ss.get("performance_log"): st.info("No agent operations performed yet."); return
        log_df = pd.DataFrame(ss.performance_log); avg_duration = log_df['duration_ms'].mean()
        st.metric("Avg. Agent Response Time", f"{avg_duration:.0f} ms")
        st.line_chart(log_df.rename(columns={'timestamp': 'Time', 'duration_ms': 'Response Time (ms)'}).set_index('Time')['Response Time (ms)'])
        with st.expander("View Raw Logs"): st.dataframe(log_df)
        if st.button("Clear Log", use_container_width=True): ss.performance_log = []; st.rerun()

def render_document_previewer(selected_file: str, ss: Dict, key_prefix: str):
    # <<< FIX: Added a unique key_prefix to make each text_area unique >>>
    if not selected_file: return
    with st.expander(f"Preview Content of: `{selected_file}`"):
        doc = ss.full_docs.get(selected_file)
        if doc:
            content = doc.page_content
            st.text_area(
                "Document Content",
                value=content[:5000] + "..." if len(content) > 5000 else content,
                height=300,
                disabled=True,
                help="Showing the first 5000 characters of the document.",
                key=f"{key_prefix}_previewer"  # <<< THE FIX IS HERE
            )
        else: st.error("Could not load document content for preview.")

# ======================================================================================
# SECTION 3: CORE ACTION HANDLERS (REAL AGENTS + FIXED COUNTERS)
# ======================================================================================
def handle_qa_submission(prompt: str, ss: Dict):
    if not prompt: return
    ss.qa_messages.append({"role": "user", "content": prompt})
    ss.usage_stats['queries_executed'] += 1
    retriever = get_retriever_from_state(ss)
    if not retriever: ss.qa_messages.append({"role": "assistant", "content": "CRITICAL ERROR: Vector Store not initialized."}); st.rerun(); return
    start_time = time.perf_counter()
    with st.spinner("Q&A Agent is thinking..."):
        response_obj = execute_qa_chain(retriever, prompt, ss.qa_messages[:-1])
        track_performance("Q&A", start_time, ss)
        assistant_message = {"role": "assistant", "content": str(response_obj.get("answer", "Sorry, an error occurred.")), "sources": response_obj.get("source_documents", [])} if isinstance(response_obj, dict) else {"role": "assistant", "content": str(response_obj), "sources": []}
        ss.qa_messages.append(assistant_message)
    st.rerun()

def handle_summarization_submission(selected_file: str, summary_length: str, ss: Dict):
    if not selected_file: st.warning("Please select a document."); return
    if doc := ss.full_docs.get(selected_file):
        ss.usage_stats['queries_executed'] += 1
        start_time = time.perf_counter()
        with st.spinner(f"Generating {summary_length} summary..."):
            ss.summary_output = execute_summarization_chain([doc], summary_length)
            track_performance("Summarization", start_time, ss)
    else: st.error(f"Error: Content for '{selected_file}' not found.")

def handle_entity_extraction_submission(selected_file: str, ss: Dict):
    if not selected_file: st.warning("Please select a document."); return
    if doc := ss.full_docs.get(selected_file):
        ss.usage_stats['queries_executed'] += 1
        start_time = time.perf_counter()
        with st.spinner(f"Extracting entities..."):
            ss.entity_output = execute_entity_extraction_chain(doc)
            track_performance("Entity Extraction", start_time, ss)
    else: st.error(f"Error: Content for '{selected_file}' not found.")

def handle_comparison_submission(selected_files: List[str], comparison_query: str, ss: Dict):
    if len(selected_files) < 2: st.warning("Please select at least two documents."); return
    if not (retriever := get_retriever_from_state(ss)): st.error("CRITICAL ERROR: Vector Store not initialized."); return
    ss.usage_stats['queries_executed'] += 1
    start_time = time.perf_counter()
    with st.spinner("Comparison Agent is analyzing..."):
        full_query = (f"Compare these docs: '{', '.join(selected_files)}'. Request: {comparison_query}")
        ss.comparison_output = execute_comparison_chain(retriever, full_query)
        track_performance("Comparison", start_time, ss)

def handle_report_submission(report_query: str, ss: Dict):
    if not report_query: st.warning("Please describe the report."); return
    if not (retriever := get_retriever_from_state(ss)): st.error("CRITICAL ERROR: Vector Store not initialized."); return
    ss.usage_stats['queries_executed'] += 1
    start_time = time.perf_counter()
    with st.spinner("Report Agent is writing..."):
        ss.report_output = execute_report_chain(retriever, report_query)
        track_performance("Report Generation", start_time, ss)

def handle_debug_submission(debug_query: str, ss: Dict):
    if not debug_query: st.warning("Please enter a query to debug."); return
    if not (retriever := get_retriever_from_state(ss)): st.error("CRITICAL ERROR: Vector Store not initialized."); return
    ss.usage_stats['queries_executed'] += 1
    start_time = time.perf_counter()
    with st.spinner("Debugging retriever..."):
        ss.debug_output = execute_debug_chain(retriever, debug_query)
        track_performance("Debug", start_time, ss)

# ======================================================================================
# SECTION 4: TAB-SPECIFIC UI RENDERING FUNCTIONS
# ======================================================================================
def render_qa_tab(ss: Dict, is_disabled: bool):
    render_tool_card("comments", "Conversational Q&A", "Ask questions and get answers sourced directly from your documents. Check 'Show Sources' to verify the AI's context.")
    chat_container = st.container(height=400)
    with chat_container:
        for msg in ss.qa_messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                if msg["role"] == "assistant" and msg.get("sources"):
                    with st.expander("Show Sources"):
                        for i, doc in enumerate(msg["sources"]):
                            st.info(f"**Source {i+1}: `{doc.metadata.get('source', 'N/A')}`**"); st.text(doc.page_content[:350] + "...")
    if prompt := st.chat_input("Ask a question...", disabled=is_disabled): handle_qa_submission(prompt, ss)

def render_summarizer_tab(ss: Dict, is_disabled: bool):
    render_tool_card("file-alt", "Document Summarizer", "Condense lengthy documents into brief or detailed overviews.")
    with st.form("summarizer_form"):
        st.subheader("Summarization Controls")
        selected_file = st.selectbox("1. Select Document:", ss.get("processed_files", []), disabled=is_disabled, help="Choose one document to summarize.", key="summarizer_select")
        summary_length = st.radio("2. Choose Length:", ["Brief", "Detailed"], horizontal=True, disabled=is_disabled)
        if st.form_submit_button("📄 Generate Summary", use_container_width=True, type="primary", disabled=is_disabled):
            ss.summary_output = None; handle_summarization_submission(selected_file, summary_length.lower(), ss)
    render_document_previewer(ss.get('summarizer_select'), ss, key_prefix="summarizer") # <<< FIX: Added unique key_prefix

def render_entity_extraction_tab(ss: Dict, is_disabled: bool):
    render_tool_card("tags", "Key Entity Extraction", "Automatically identify and categorize People, Organizations, Locations, and more.")
    with st.form("entity_form"):
        st.subheader("Extraction Controls")
        selected_file = st.selectbox("Select Document to Analyze:", ss.get("processed_files", []), disabled=is_disabled, key="entity_select")
        if st.form_submit_button("🏷️ Extract Entities", use_container_width=True, type="primary", disabled=is_disabled):
            ss.entity_output = None; handle_entity_extraction_submission(selected_file, ss)
    render_document_previewer(ss.get('entity_select'), ss, key_prefix="entity") # <<< FIX: Added unique key_prefix

def render_comparison_tab(ss: Dict, is_disabled: bool):
    render_tool_card("scale-balanced", "Comparative Analysis", "Select multiple documents and ask the AI to analyze their similarities and differences.")
    with st.form("comparison_form"):
        st.subheader("Comparison Controls")
        selected_files = st.multiselect("1. Select 2 or more Documents:", ss.get("processed_files", []), disabled=is_disabled)
        comparison_query = st.text_area("2. Specify Comparison Focus:", placeholder="e.g., 'Compare the financial projections and stated risks.'", height=100, disabled=is_disabled)
        if st.form_submit_button("⚖️ Generate Comparison", use_container_width=True, type="primary", disabled=is_disabled):
            ss.comparison_output = None; handle_comparison_submission(selected_files, comparison_query, ss)
    render_results_container("Comparison", "comparison_output", ss)

def render_report_tab(ss: Dict, is_disabled: bool):
    render_tool_card("chart-line", "In-Depth Report Generation", "Synthesize information from all documents to generate a comprehensive report.")
    with st.form("report_form"):
        st.subheader("Report Controls")
        report_query = st.text_area("Describe the Report to Generate:", placeholder="e.g., 'Generate a SWOT analysis based on the provided business plans.'", height=120, disabled=is_disabled)
        if st.form_submit_button("📊 Generate Report", use_container_width=True, type="primary", disabled=is_disabled):
            ss.report_output = None; handle_report_submission(report_query, ss)
    render_results_container("Report", "report_output", ss)

def render_debug_tab(ss: Dict, is_disabled: bool):
    render_tool_card("bug", "Retriever Debugger", "Inspect the exact context the AI sees for a query. Essential for diagnosing unexpected Q&A answers.")
    with st.form("debug_form"):
        st.subheader("Debugger Controls")
        debug_query = st.text_input("Enter Query to Debug:", placeholder="e.g., 'What were the Q3 earnings?'", disabled=is_disabled)
        if st.form_submit_button("🔍 Debug Query", use_container_width=True, type="primary", disabled=is_disabled):
            ss.debug_output = None; handle_debug_submission(debug_query, ss)
    render_results_container("Debug Log", "debug_output", ss)

# ======================================================================================
# SECTION 5: MAIN PAGE CONDUCTOR
# ======================================================================================
def display_analyzer_page(ss: Dict):
    st.title("🧠 Analyzer Workstation"); st.markdown("Your intelligent hub for interacting with documents. Process files via the sidebar, then use the tools below."); st.markdown("---")
    render_performance_sidebar(ss)
    is_disabled = not ss.get("processed_files")
    if is_disabled: st.info("💡 Please process documents using the sidebar to activate these analysis tools.", icon="👆")
    tabs = st.tabs(["💬 Q&A", "📄 Summarizer", "🏷️ Entities", "⚖️ Compare", "📊 Report", "🐞 Debug"])
    render_functions = [render_qa_tab, render_summarizer_tab, render_entity_extraction_tab, render_comparison_tab, render_report_tab, render_debug_tab]
    for tab, func in zip(tabs, render_functions):
        with tab: func(ss, is_disabled)